{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82843a3e-0776-4c72-a455-63fbb4dccbe4",
   "metadata": {},
   "source": [
    "# Comparing MLP and Convolutional Models\n",
    "\n",
    "In this week's homework, we'll use PyTorch to compare the performance of multi-layer perceptrons (MLP's; the kind of model we've looked at so far) against _convolutional_ neural networks (CNN's). We'll talk about convolutions in class on Monday, but you can get started on the MLP part of the homework as soon as you're ready. For our comparison we'll go back to the CIFAR-10 dataset, since it's a bit less chaotic than CIFAR-100.\n",
    "\n",
    "As with the last homework, I have some guidelines about what parts of the homework are necessary for different grades:\n",
    "\n",
    "- The basic version, for a C, is to define and train an MLP and a CNN.\n",
    "- On top of that, the B level work requires you to analyze your results a bit. I'll describe this in more detail later in the notebook after the code that sets up and trains the networks.\n",
    "- For an A, you'll need to finish the activation map visualization in the last section of this notebook. More details on that later on.\n",
    "\n",
    "The rest of the document is organized into sections which are labeled with the grade they correspond to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24cd25-bc95-4507-971b-e407c7b8b468",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "The first section of the notebook gets the dataset and sets up the transforms we need. The code in this section is complete, although you may need to change the dataset path or change the `transform` definition to match your version of torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0090403a-c772-4aa2-aa57-484713eaf063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineRenderer.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe40599-58d5-4920-9495-13f356e7c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToImageTensor(),\n",
    "    transforms.ConvertImageDtype(),\n",
    "    # Depending on your torchvision version you may need to change these:\n",
    "    # - If you don't have torchvision.transforms.v2, then import torchvision.transforms\n",
    "    #   instead and use ToTensor() to replace _both_ of the transforms above.\n",
    "    # - If you have v2 but it says ToImage() is undefined, then use ToImageTensor() instead.\n",
    "])\n",
    "\n",
    "# If you already have the CIFAR10 data downloaded from the in-class notebook, you can change the path here\n",
    "# to point to it so you avoid downloading a second copy.\n",
    "cifar = torchvision.datasets.CIFAR10(\"../data/torch/cifar\", download=True, transform=transform)\n",
    "train_size = int(0.8 * len(cifar))\n",
    "train_data, valid_data = torch.utils.data.random_split(cifar, [train_size, len(cifar) - train_size])\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff7fa3b-fe94-4ad5-a11d-0af917329dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "normalize = transforms.Normalize(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ab889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else: \n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"Device:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c144a-b2ae-455b-a636-ebd7a28806db",
   "metadata": {},
   "source": [
    "## Models and Training (C)\n",
    "\n",
    "First, define an MLP model for the CIFAR dataset. An MLP, also called a fully-connected network, consists of linear computations alternated with nonlinear activation functions, just like every network we've looked at in this class so far. This is very similar to what we did in clas on Wednesday and lab on Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5917c17-dd34-4d98-82aa-47a2734f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, arch=[256, 128], activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        arch.insert(0, 3 * 32 * 32)\n",
    "        arch.append(100)\n",
    "        for i in range(len(arch) - 1):\n",
    "            layers.append(nn.Linear(arch[i], arch[i+1]))\n",
    "            if i < len(arch) - 2:\n",
    "                layers.append(activation())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.reshape(-1, 3 * 32 * 32)).squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0cb82-472e-4c23-8e83-b30df497de8a",
   "metadata": {},
   "source": [
    "Now let's define a training function for our MLP. As usual, you may want to add more arguments to the training function. For the latter parts of the notebook, it will be helpful if your training function returns both the model and a list of the training and validation accuracies for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56a7791e-a629-4776-b259-c475dae86722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_class=MLP, arch= [256,128], lr=1e-3, epochs=10, batch_size=64, momentum=0.9, reg=1e-5, activation=nn.ReLU, use_augmentation=False, aug_params=None):\n",
    " \n",
    "    data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False) # why shuffle = false here?\n",
    "\n",
    "    if use_augmentation:\n",
    "        augments = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(aug_params['flip_prob']),\n",
    "            transforms.RandomGrayscale(aug_params['grayscale_prob']),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=(aug_params['bright_min'], aug_params['bright_max']),\n",
    "                contrast=0,\n",
    "                saturation=0,\n",
    "                hue=0),\n",
    "            transforms.RandomCrop(\n",
    "                size=32,\n",
    "                padding=aug_params['shift_size'],\n",
    "                fill=cifar_mean)\n",
    "        ])\n",
    "\n",
    "\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "\n",
    "    if model_class == MLP:\n",
    "        model = MLP(arch=arch, activation=activation).to(device)\n",
    "    if model_class == CNN:\n",
    "        print(device)\n",
    "        model = CNN().to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), momentum=momentum, lr=lr, weight_decay=reg)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train() # why do we need this?\n",
    "        accs = []\n",
    "        for batch_xs, batch_ys in data_loader:\n",
    "            batch_xs = batch_xs.to(device)\n",
    "            batch_ys = batch_ys.to(device)\n",
    "\n",
    "            if use_augmentation:\n",
    "                batch_xs = augments(batch_xs).to(device)\n",
    "\n",
    "            preds = model(normalize(batch_xs))\n",
    "            loss_val = loss(preds, batch_ys)\n",
    "            accs.append((preds.argmax(dim=1) == batch_ys).float().mean())\n",
    "\n",
    "            # Update\n",
    "            opt.zero_grad()\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        # average training accuracy\n",
    "        train_accs.append(torch.tensor(accs).mean().item())\n",
    "\n",
    "        # start over for eval stage\n",
    "        model.eval()\n",
    "        accs = []\n",
    "        for batch_xs, batch_ys in valid_loader:\n",
    "            batch_xs = batch_xs.to(device)\n",
    "            batch_ys = batch_ys.to(device)\n",
    "            valid_preds = model(normalize(batch_xs))\n",
    "            accs.append((valid_preds.argmax(dim=1) == batch_ys).float().mean())\n",
    "        \n",
    "        valid_accs.append(torch.tensor(accs).mean().item())\n",
    "        \n",
    "        \n",
    "        print(\"Epoch:\", i, \"Acc:\", valid_accs[-1])\n",
    "    return model, train_accs, valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db839eaf-5da8-479b-8280-4b67556bc7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "Cell \u001b[0;32mIn[32], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_class, arch, lr, epochs, batch_size, momentum, reg, activation, use_augmentation, aug_params)\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# why do we need this?\u001b[39;00m\n\u001b[1;32m     35\u001b[0m accs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 36\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_xs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_ys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_xs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_xs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_ys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_ys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m sample \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:37\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     35\u001b[0m     flat_inputs, spec \u001b[38;5;241m=\u001b[39m tree_flatten(inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m inputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     40\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     41\u001b[0m         [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     42\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:25\u001b[0m, in \u001b[0;36mTransform._check_inputs\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     23\u001b[0m     _log_api_usage_once(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, flat_inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, flat_inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aug_params = {\n",
    "    'flip_prob': 0.5,\n",
    "    'grayscale_prob': 0.1,\n",
    "    'bright_min': 0.9,\n",
    "    'bright_max': 1.1,\n",
    "    'shift_size': 2\n",
    "}\n",
    "\n",
    "mlp_model, mlp_train_accs, mlp_valid_accs = train(model_class=MLP, \n",
    "                                                    arch= [256,128],\n",
    "                                                    lr=1e-3, \n",
    "                                                    epochs=5, \n",
    "                                                    batch_size=64,\n",
    "                                                    momentum=0.9,\n",
    "                                                    reg=1e-5, \n",
    "                                                    activation=nn.ReLU, \n",
    "                                                    use_augmentation=True, \n",
    "                                                    aug_params=aug_params\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08258b8d",
   "metadata": {},
   "source": [
    "### Vizualize and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f34e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz predictions\n",
    "\n",
    "# fig, axs = plt.subplots(1, 5, figsize=(8, 8))\n",
    "# for i in range(5):\n",
    "#     r = torch.randint(len(cifar), (1,)).item()\n",
    "#     axs[i].imshow(cifar[r][0].numpy().transpose(1, 2, 0))\n",
    "#     preds = nn.functional.softmax(mlp_model(normalize(cifar[r][0])), dim=0)\n",
    "#     c = torch.argmax(preds)\n",
    "#     axs[i].set_title(\"{}: ({:.2f})\".format(classes[c], preds[c]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ea7a71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training error against the validation error (something off)\n",
    "\n",
    "smooth_train = np.convolve(mlp_train_accs, np.ones(2) / 2, mode='valid')\n",
    "smooth_valid = np.convolve(mlp_valid_accs, np.ones(2) / 2, mode='valid')\n",
    "plt.plot(range(len(smooth_train)), 1-smooth_train, label='training error')\n",
    "plt.plot(range(len(smooth_valid)), 1-smooth_valid, label='validation error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd0d38-aa25-4d97-bd63-0e61c4320448",
   "metadata": {},
   "source": [
    "Now you can define a CNN for the same task. NOTE: for the visualization at the end of the notebook you'll need to have access to the hidden layer outputs when running the network. If you plan to work on that part I would recommend setting up your CNN so that the `forward` function returns both the predictions and the values at the hidden layers. If you don't, there are some hacky ways to get around this later though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "608fb146-c6eb-404b-9ac5-85c1e35675ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(3, 8, 3, padding=3)\n",
    "        self.a1 = activation()\n",
    "        self.c2 = nn.Conv2d(8, 16, 3, groups=2, padding=1, stride=2)\n",
    "        self.a2 = activation()\n",
    "        self.c3 = nn.Conv2d(16, 32, 3, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 =  nn.Linear(32*9*9, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.c1(x)\n",
    "        h2 = self.a1(h1)\n",
    "        h3 = self.c2(h2)\n",
    "        h4 = self.a2(h3)\n",
    "        h5 = self.c3(h4)\n",
    "        h6 = self.flatten(h5)\n",
    "        h7 = self.l1(h6)\n",
    "        return h7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805ea5f-607d-45b9-b32c-84c6248b7b41",
   "metadata": {},
   "source": [
    "Now you can train your CNN. You should be able use the same `train` function but pass it `model_class=CNN` (you may need to change some other hyperparameters to get good results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ded6d3f-5f6f-4a27-8e24-5cc6f770d6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (64x2048 and 2592x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[32], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_class, arch, lr, epochs, batch_size, momentum, reg, activation, use_augmentation, aug_params)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_augmentation:\n\u001b[1;32m     41\u001b[0m     batch_xs \u001b[38;5;241m=\u001b[39m augments(batch_xs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_xs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss(preds, batch_ys)\n\u001b[1;32m     45\u001b[0m accs\u001b[38;5;241m.\u001b[39mappend((preds\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m batch_ys)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 20\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m h5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc3(h4)\n\u001b[1;32m     19\u001b[0m h6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(h5)\n\u001b[0;32m---> 20\u001b[0m h7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh6\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h7\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-class/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (64x2048 and 2592x10)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnn_model, cnn_train_accs, cnn_valid_accs = train(model_class=CNN, \n",
    "                                                    lr=2e-3, \n",
    "                                                    epochs=2, \n",
    "                                                    batch_size=64,\n",
    "                                                    momentum=0.9,\n",
    "                                                    reg=1e-5, \n",
    "                                                    activation=nn.ReLU, \n",
    "                                                    use_augmentation=True, \n",
    "                                                    aug_params=aug_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb01db-f89f-4a53-a5b1-77faf2bd8dc2",
   "metadata": {},
   "source": [
    "Given how long it takes to train the CNN model, now might be a good time to talk about saving and loading models. The cell below will save your CNN model to the file `cnn_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b2210-1745-4283-89ce-9bfca9653fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn_model.state_dict(), \"cnn_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ea796-3666-4bee-90a7-9c9e32c06b13",
   "metadata": {},
   "source": [
    "If you have the file `cnn_model.pt` on your system, then you can load it with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8208a-8b61-4220-b095-3d56731130be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CNN()\n",
    "loaded_model.load_stat_dict(torch.load(\"cnn_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best parameters for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651538a-987a-4d96-afed-c6d38825d0f1",
   "metadata": {},
   "source": [
    "## Analysis (B)\n",
    "\n",
    "In this section, we'll do some minor analysis of the results of our experiment above. Let's compare\n",
    "\n",
    "- the time taken to train each model,\n",
    "- the validation accuracy over time,\n",
    "- the total number of parameters in each model.\n",
    "\n",
    "That last measurement, the total number of parameters, is something you'll need to compute pased on your network architecture. When I say number of parameters here, I'm referring to the total number of dimensions in the parameter space. That is, the total number of individual real numbers in our parameters. For example, a matrix of size 100x10 has 1,000 parameters.\n",
    "\n",
    "I'm not asking for anything specific here, I'm just looking for you to think about the models we're using and their relative merits. If you think of something else that might be useful to compare the two networks, mention that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc92e0-25ab-4fbe-9b04-3d56b82af766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this cell for any code related to your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812b240-336d-4aa2-aee4-0ff9ff13ee11",
   "metadata": {},
   "source": [
    "## Interpretation (A)\n",
    "\n",
    "In this section, we'll explore our CNN and try to understand how it learns to recognize different objects. There are several approaches to this problem. For today we'll look at a simple one that only works for the first convolutional layer. If we plot the weights of the layer as image data, we can visualize the kinds of patterns the convolution is scanning for. In order to do this, you'll need to normalize each kernel to the range [0, 1], then transpose the axis to format the weights as image data, then use the `imshow` function from `matplotlib.pyplot`. The weights of a convolutional layer `conv` are stored as a tensor of shape `Cin, Cout, H, W` in `conv.weight`.\n",
    "\n",
    "If you have a torch tensor `x` and you want to display it with `imshow` you'll need to convert it to numpy by calling `x.detach().numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fe436-5667-4f21-83a0-3373714c5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your first convolutional layer has fewer than 16 output channels then you'll need to\n",
    "# change the number of plots here.\n",
    "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        c = 4 * i + j\n",
    "        YOUR_CODE_HERE\n",
    "        axs[i,j].imshow(YOUR_CODE_HERE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adfe535-7f4f-442a-b645-475237a5d2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
